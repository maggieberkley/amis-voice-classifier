{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtTmZYbErQgxd7RVfunF6Q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation: NTU Corpus in Amis, English, Chinese, and glossing"
      ],
      "metadata": {
        "id": "Iqtbsliiui_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u45OoQtlS2Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import glob"
      ],
      "metadata": {
        "id": "IZADnKr0TCC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Get data and put it into the right format"
      ],
      "metadata": {
        "id": "yUGeCdqZuttg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists of all files in Amis, English, Chinese, and glossing\n",
        "ntu_files_am = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.fo\")\n",
        "ntu_files_en = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.en\")\n",
        "ntu_files_zh = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.zh\")\n",
        "ntu_files_gloss = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.gloss\")\n",
        "\n",
        "# Sort file names in alphabetical order\n",
        "ntu_files_am = sorted(ntu_files_am)\n",
        "ntu_files_en = sorted(ntu_files_en)\n",
        "ntu_files_zh = sorted(ntu_files_zh)\n",
        "ntu_files_gloss = sorted(ntu_files_gloss)\n",
        "\n",
        "# Get list of just file names for later\n",
        "filenames = [re.sub(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/\", \"\", name) for name in ntu_files_am]\n",
        "filenames = [re.sub(\".fo\", \"\", name) for name in filenames]"
      ],
      "metadata": {
        "id": "fyoMF4DUTDYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filenames)\n",
        "print(len(filenames))\n",
        "print(filenames[1])"
      ],
      "metadata": {
        "id": "8nfbvobAtxMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first and last file name in each list\n",
        "print(ntu_files_am[0])\n",
        "print(ntu_files_en[0])\n",
        "print(ntu_files_zh[0])\n",
        "print(ntu_files_gloss[0])\n",
        "\n",
        "print(ntu_files_am[-1])\n",
        "print(ntu_files_en[-1])\n",
        "print(ntu_files_zh[-1])\n",
        "print(ntu_files_gloss[-1])\n",
        "\n",
        "# Check length of each list\n",
        "print(len(ntu_files_am))\n",
        "print(len(ntu_files_en))\n",
        "print(len(ntu_files_zh))\n",
        "print(len(ntu_files_gloss))"
      ],
      "metadata": {
        "id": "l2VsYfTJTHMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each story, combine the Amis, English, Chinese, and gloss into a list\n",
        "# This way we can convert it into XML\n",
        "ntu_big_list = []\n",
        "for ntu_file in ntu_files_am:\n",
        "\n",
        "  # The order of elements will be\n",
        "  # Amis (standardized orthography), Amis, English, Chinese, gloss\n",
        "  ntu_list = []\n",
        "\n",
        "  # Amis file (we're not going to clean this one)\n",
        "  filename = ntu_file[:-2]\n",
        "  f1 = open(ntu_file, \"r\")\n",
        "  file_am = f1.read().strip(\"\\n\")\n",
        "\n",
        "  # Split on newline\n",
        "  list_am = file_am.split(\"\\n\")\n",
        "  # Strip leading spaces\n",
        "  list_am = [sentence.lstrip() for sentence in list_am]\n",
        "\n",
        "  # Clean the Amis data for standardized orthography\n",
        "  file_am_standard = re.sub(r\"[\\\\_,.\\[\\]]+\", \"\", file_am)\n",
        "  file_am_standard = re.sub(\"==\", \"\", file_am_standard)\n",
        "  file_am_standard = re.sub(r\"[’‘]\", \"'\", file_am_standard)\n",
        "  file_am_standard = re.sub(r\"<L2\\w\", \"\", file_am_standard)\n",
        "  file_am_standard = re.sub(r\"L2\\w>\", \"\", file_am_standard)\n",
        "  # Amis with standardized orthography (replace o with u)\n",
        "  file_am_standard = re.sub(\"o\", \"u\", file_am_standard)\n",
        "\n",
        "  # Split on newline\n",
        "  list_am_standard = file_am_standard.split(\"\\n\")\n",
        "  # Strip leading spaces\n",
        "  list_am_standard = [sentence.lstrip() for sentence in list_am_standard]\n",
        "\n",
        "  # Add standard Amis then original Amis to list\n",
        "  ntu_list.append(list_am_standard)\n",
        "  ntu_list.append(list_am)\n",
        "\n",
        "  # English file\n",
        "  f2 = open(filename + \"en\", \"r\")\n",
        "  file_en = f2.read().strip(\"\\n\")\n",
        "  list_en = file_en.split(\"\\n\")\n",
        "  list_en = [sentence.lstrip() for sentence in list_en]\n",
        "  ntu_list.append(list_en)\n",
        "\n",
        "  # Chinese file\n",
        "  f3 = open(filename + \"zh\", \"r\")\n",
        "  file_zh = f3.read().strip(\"\\n\")\n",
        "  list_zh = file_zh.split(\"\\n\")\n",
        "  list_zh = [sentence.lstrip() for sentence in list_zh]\n",
        "  ntu_list.append(list_zh)\n",
        "\n",
        "  # Glossing\n",
        "  f4 = open(filename + \"gloss\", \"r\")\n",
        "  file_gloss = f4.read().strip(\"\\n\")\n",
        "  list_gloss = file_gloss.split(\"\\n\")\n",
        "  list_gloss = [sentence.lstrip() for sentence in list_gloss]\n",
        "  ntu_list.append(list_gloss)\n",
        "\n",
        "  ntu_big_list.append(ntu_list)"
      ],
      "metadata": {
        "id": "C8Y2OO4JTKi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to make sure it lines up\n",
        "print(ntu_big_list[0][0][:3])\n",
        "print(ntu_big_list[0][1][:3])\n",
        "print(ntu_big_list[0][2][:3])\n",
        "print(ntu_big_list[0][3][:3])\n",
        "print(ntu_big_list[0][4][:3])\n",
        "print()\n",
        "\n",
        "print(ntu_big_list[-1][0][:3])\n",
        "print(ntu_big_list[-1][1][:3])\n",
        "print(ntu_big_list[-1][2][:3])\n",
        "print(ntu_big_list[-1][3][:3])\n",
        "print(ntu_big_list[-1][4][:3])"
      ],
      "metadata": {
        "id": "xCe4YWFNUfEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check length of big list\n",
        "print(len(ntu_files_am))\n",
        "print(len(ntu_big_list))\n",
        "\n",
        "# Check to make sure all languages have the same number of lines\n",
        "for i in range(len(ntu_big_list)):\n",
        "  story = ntu_big_list[i]\n",
        "  if not (len(story[0]) == len(story[1]) == len(story[2]) == len(story[3]) == len(story[4])):\n",
        "    print(ntu_files_am[i])\n",
        "    print(len(story[0]))\n",
        "    print(len(story[1]))\n",
        "    print(len(story[2]))\n",
        "    print(len(story[3]))\n",
        "    print(len(story[4]))"
      ],
      "metadata": {
        "id": "t9n7VI3ZS2jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3UC2KffJaFT"
      },
      "outputs": [],
      "source": [
        "# Now our NTU data is ready for XML\n",
        "# Each source list contains multiple text lists\n",
        "# Each text list includes a list for each language\n",
        "# Each list for each language contains sentences\n",
        "\n",
        "# The format of ntu_big_list is as follows\n",
        "# ntu_big_list[story][language][sentence]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: XML\n"
      ],
      "metadata": {
        "id": "XnXf84jUu2JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import xml.dom.minidom"
      ],
      "metadata": {
        "id": "dnyJ_O2Ra6Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the code for putting both the Amis and English translations into XML\n",
        "# For each story, create a root and a TEXT subelement under root\n",
        "for i in range(len(ntu_big_list)):\n",
        "  root = ET.Element(\"root\")\n",
        "  text_element = ET.SubElement(root, \"TEXT\", {\"xml:lang\": \"amis1246\", \"source\": f\"ntu_{filenames[i]}\", \"audio\": f\"{filenames[i]}.mp3\"})\n",
        "\n",
        "  # For each sentence in the story, create the S element and its child elements\n",
        "  for j in range(len(ntu_big_list[i][0])):\n",
        "\n",
        "    # First get the FORM (Amis sentence in standard orthography)\n",
        "    s_element = ET.SubElement(text_element, \"S\", id=f\"S{j}\")\n",
        "    form_element = ET.SubElement(s_element, \"FORM\", kindOf=\"standard\")\n",
        "    std_sentence = ntu_big_list[i][0][j]\n",
        "    std_sentence = re.sub(r\"[-=<>]\", \"\", std_sentence)\n",
        "    form_element.text = std_sentence\n",
        "\n",
        "    # Now get the Amis in original orthography\n",
        "    form_element = ET.SubElement(s_element, \"FORM\", kindOf=\"original\")\n",
        "    form_element.text = ntu_big_list[i][1][j]\n",
        "\n",
        "    # Now get the English\n",
        "    transl_element = ET.SubElement(s_element, \"TRANSL\", {\"xml:lang\": \"stan1293\"})\n",
        "    transl_element.text = ntu_big_list[i][2][j]\n",
        "\n",
        "    # Now get the Chinese\n",
        "    transl_element = ET.SubElement(s_element, \"TRANSL\", {\"xml:lang\": \"mand1415\"})\n",
        "    transl_element.text = ntu_big_list[i][3][j]\n",
        "\n",
        "    # Now get the glossing\n",
        "    transl_element = ET.SubElement(s_element, \"TRANSL\", {\"xml:lang\": \"gloss\"})\n",
        "    ntu_big_list[i][4][j] = re.sub(\"\\s\\s+\", \" \", ntu_big_list[i][4][j])\n",
        "    transl_element.text = ntu_big_list[i][4][j]\n",
        "\n",
        "    # Now get the words (from the standard Amis sentences)\n",
        "    words = ntu_big_list[i][0][j].split()\n",
        "    glossing = ntu_big_list[i][4][j].split()\n",
        "\n",
        "    for k in range(len(words)):\n",
        "      w_element = ET.SubElement(s_element, \"W\", id=f\"S{j}W{k}\")\n",
        "      wform_element = ET.SubElement(w_element, \"FORM\")\n",
        "      std_word = words[k]\n",
        "      std_word = re.sub(r\"[-=<>]\", \"\", std_word)\n",
        "      std_word = re.sub(r\"[.!?]\", \"\", std_word)\n",
        "      wform_element.text = std_word\n",
        "\n",
        "      # Now get the morphemes for each word\n",
        "      # Usually separated by <>, =, -, or space\n",
        "      # Split on =, –, and space\n",
        "      morphs = re.split(r\"[=\\-\\s]\", words[k])\n",
        "      glosses = re.split(r\"[=\\-\\s]\", glossing[k])\n",
        "      # Deal with infixes\n",
        "      morph_match = re.search(r\"<.+>\", words[k])\n",
        "      gloss_match = re.search(r\"<.+>\", glossing[k])\n",
        "      if morph_match:\n",
        "        morph_infix = morph_match.group()\n",
        "        morph_infix_clean = re.sub(\"<\", \"\", morph_infix)\n",
        "        morph_infix_clean = re.sub(\">\", \"\", morph_infix_clean)\n",
        "        morphs.insert(0, morph_infix_clean)\n",
        "        for m in range(len(morphs)):\n",
        "          morphs[m] = re.sub(morph_infix, \"\", morphs[m])\n",
        "      if gloss_match:\n",
        "        gl_infix = gloss_match.group()\n",
        "        gl_infix_clean = re.sub(\"<\", \"\", gl_infix)\n",
        "        gl_infix_clean = re.sub(\">\", \"\", gl_infix_clean)\n",
        "        glosses.insert(0, gl_infix_clean)\n",
        "        for m in range(len(glosses)):\n",
        "          glosses[m] = re.sub(gl_infix, \"\", glosses[m])\n",
        "\n",
        "      for m in range(len(morphs)):\n",
        "        m_element = ET.SubElement(w_element, \"M\", id=f\"S{j}W{k}M{m}\")\n",
        "        # Create FORM within each morpheme\n",
        "        mform_element = ET.SubElement(m_element, \"FORM\")\n",
        "        mform_element.text = morphs[m]\n",
        "        # Create TRANS for gloss within morpheme\n",
        "        try:\n",
        "          glosstransl_element = ET.SubElement(m_element, \"TRANSL\")\n",
        "          glosstransl_element.text = glosses[m]\n",
        "        except:\n",
        "          print(filenames[i], \", Sentence:\", j, \", Word:\", k, \"Morpheme:\", m)\n",
        "\n",
        "  # Generate XML tree\n",
        "  tree = ET.ElementTree(root)\n",
        "\n",
        "  # This is what will make the XML look \"pretty\" (readable,\n",
        "  # with newlines, indenting) when you write it out to a file.\n",
        "  xml_str = xml.dom.minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"    \")\n",
        "\n",
        "  # This does the writing to the file.\n",
        "  with open(f\"/content/drive/MyDrive/Part1/NTUV4/ntu_{filenames[i]}.xml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(xml_str)"
      ],
      "metadata": {
        "id": "go6DpuA5soCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}