{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtTmZYbErQgxd7RVfunF6Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maggieberkley/amis-voice-classifier/blob/main/NTU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation: NTU Corpus in Amis, English, Chinese, and glossing"
      ],
      "metadata": {
        "id": "Iqtbsliiui_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u45OoQtlS2Nq",
        "outputId": "2d4587ec-31af-4ea9-d6dd-d18069278e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import glob"
      ],
      "metadata": {
        "id": "IZADnKr0TCC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Get data and put it into the right format"
      ],
      "metadata": {
        "id": "yUGeCdqZuttg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists of all files in Amis, English, Chinese, and glossing\n",
        "ntu_files_am = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.fo\")\n",
        "ntu_files_en = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.en\")\n",
        "ntu_files_zh = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.zh\")\n",
        "ntu_files_gloss = glob.glob(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/*.gloss\")\n",
        "\n",
        "# Sort file names in alphabetical order\n",
        "ntu_files_am = sorted(ntu_files_am)\n",
        "ntu_files_en = sorted(ntu_files_en)\n",
        "ntu_files_zh = sorted(ntu_files_zh)\n",
        "ntu_files_gloss = sorted(ntu_files_gloss)\n",
        "\n",
        "# Get list of just file names for later\n",
        "filenames = [re.sub(\"/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/\", \"\", name) for name in ntu_files_am]\n",
        "filenames = [re.sub(\".fo\", \"\", name) for name in filenames]"
      ],
      "metadata": {
        "id": "fyoMF4DUTDYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filenames)\n",
        "print(len(filenames))\n",
        "print(filenames[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nfbvobAtxMk",
        "outputId": "587e644d-e765-4589-bc8f-c358e51a35b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Amis_Conv-farming_marang_furayang', 'Amis_Conv-talking_tamih_panay', 'Amis_Nr-crab_panay', 'Amis_Nr-custom_Liu', 'Amis_Nr-frog_cinhua', 'Amis_Nr-frog_minzhu', 'Amis_Nr-frog_ofad', 'Amis_Nr-frog_zuomei', 'Amis_Nr-intro_Panay', 'Amis_Nr-intro_ofad', 'Amis_Nr-intro_tamih', 'Amis_Nr-peanut_panay', 'Amis_Nr-pear_cinhua', 'Amis_Nr-pear_lungi', 'Amis_Nr-pear_minzhu', 'Amis_Nr-pear_panay', 'Amis_Nr-pear_tamih', 'Amis_Nr-pear_zuomei']\n",
            "18\n",
            "Amis_Conv-talking_tamih_panay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first and last file name in each list\n",
        "print(ntu_files_am[0])\n",
        "print(ntu_files_en[0])\n",
        "print(ntu_files_zh[0])\n",
        "print(ntu_files_gloss[0])\n",
        "\n",
        "print(ntu_files_am[-1])\n",
        "print(ntu_files_en[-1])\n",
        "print(ntu_files_zh[-1])\n",
        "print(ntu_files_gloss[-1])\n",
        "\n",
        "# Check length of each list\n",
        "print(len(ntu_files_am))\n",
        "print(len(ntu_files_en))\n",
        "print(len(ntu_files_zh))\n",
        "print(len(ntu_files_gloss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2VsYfTJTHMo",
        "outputId": "41cddb1f-490c-4f14-dea6-78f7241f1acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Conv-farming_marang_furayang.fo\n",
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Conv-farming_marang_furayang.en\n",
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Conv-farming_marang_furayang.zh\n",
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Conv-farming_marang_furayang.gloss\n",
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Nr-pear_zuomei.fo\n",
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Nr-pear_zuomei.en\n",
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Nr-pear_zuomei.zh\n",
            "/content/drive/MyDrive/FormosanResources/NTU_Formosan/Amis/Amis_Nr-pear_zuomei.gloss\n",
            "18\n",
            "18\n",
            "18\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each story, combine the Amis, English, Chinese, and gloss into a list\n",
        "# This way we can convert it into XML\n",
        "ntu_big_list = []\n",
        "for ntu_file in ntu_files_am:\n",
        "\n",
        "  # The order of elements will be\n",
        "  # Amis (standardized orthography), Amis, English, Chinese, gloss\n",
        "  ntu_list = []\n",
        "\n",
        "  # Amis file (we're not going to clean this one)\n",
        "  filename = ntu_file[:-2]\n",
        "  f1 = open(ntu_file, \"r\")\n",
        "  file_am = f1.read().strip(\"\\n\")\n",
        "\n",
        "  # Split on newline\n",
        "  list_am = file_am.split(\"\\n\")\n",
        "  # Strip leading spaces\n",
        "  list_am = [sentence.lstrip() for sentence in list_am]\n",
        "\n",
        "  # Clean the Amis data for standardized orthography\n",
        "  file_am_standard = re.sub(r\"[\\\\_,.\\[\\]]+\", \"\", file_am)\n",
        "  file_am_standard = re.sub(\"==\", \"\", file_am_standard)\n",
        "  file_am_standard = re.sub(r\"[’‘]\", \"'\", file_am_standard)\n",
        "  file_am_standard = re.sub(r\"<L2\\w\", \"\", file_am_standard)\n",
        "  file_am_standard = re.sub(r\"L2\\w>\", \"\", file_am_standard)\n",
        "  # Amis with standardized orthography (replace o with u)\n",
        "  file_am_standard = re.sub(\"o\", \"u\", file_am_standard)\n",
        "\n",
        "  # Split on newline\n",
        "  list_am_standard = file_am_standard.split(\"\\n\")\n",
        "  # Strip leading spaces\n",
        "  list_am_standard = [sentence.lstrip() for sentence in list_am_standard]\n",
        "\n",
        "  # Add standard Amis then original Amis to list\n",
        "  ntu_list.append(list_am_standard)\n",
        "  ntu_list.append(list_am)\n",
        "\n",
        "  # English file\n",
        "  f2 = open(filename + \"en\", \"r\")\n",
        "  file_en = f2.read().strip(\"\\n\")\n",
        "  list_en = file_en.split(\"\\n\")\n",
        "  list_en = [sentence.lstrip() for sentence in list_en]\n",
        "  ntu_list.append(list_en)\n",
        "\n",
        "  # Chinese file\n",
        "  f3 = open(filename + \"zh\", \"r\")\n",
        "  file_zh = f3.read().strip(\"\\n\")\n",
        "  list_zh = file_zh.split(\"\\n\")\n",
        "  list_zh = [sentence.lstrip() for sentence in list_zh]\n",
        "  ntu_list.append(list_zh)\n",
        "\n",
        "  # Glossing\n",
        "  f4 = open(filename + \"gloss\", \"r\")\n",
        "  file_gloss = f4.read().strip(\"\\n\")\n",
        "  list_gloss = file_gloss.split(\"\\n\")\n",
        "  list_gloss = [sentence.lstrip() for sentence in list_gloss]\n",
        "  ntu_list.append(list_gloss)\n",
        "\n",
        "  ntu_big_list.append(ntu_list)"
      ],
      "metadata": {
        "id": "C8Y2OO4JTKi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to make sure it lines up\n",
        "print(ntu_big_list[0][0][:3])\n",
        "print(ntu_big_list[0][1][:3])\n",
        "print(ntu_big_list[0][2][:3])\n",
        "print(ntu_big_list[0][3][:3])\n",
        "print(ntu_big_list[0][4][:3])\n",
        "print()\n",
        "\n",
        "print(ntu_big_list[-1][0][:3])\n",
        "print(ntu_big_list[-1][1][:3])\n",
        "print(ntu_big_list[-1][2][:3])\n",
        "print(ntu_big_list[-1][3][:3])\n",
        "print(ntu_big_list[-1][4][:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCe4YWFNUfEV",
        "outputId": "a31d0c35-2758-472c-b56b-7f7e473b21a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"e ma-sa-ma'an=tu kiya panay tu pi-lisu' isu tayra i uma'\", \"ca=hu kaku pi-lisu'\", \"pi-ala haca XX aku tuni kalitang la'enu nuya XX lupas\"]\n",
            "['e ma-sa-ma’an=tu kiya panay,\\\\ tu pi-liso’ isu,\\\\ tayra i oma’.\\\\', 'ca=ho kaku pi-liso’.\\\\', '[pi-ala haca],\\\\ [XX],\\\\ aku tuni kalitang,\\\\ [la’enu nuya==],\\\\ [XX],\\\\ lopas.\\\\']\n",
            "['What happened to the rice last time you went to the farm?', 'I haven’t checked yet.', 'I only picked the snap beans under the peach trees.']\n",
            "['你去田裡看水稻怎麼樣了。', '我還沒有去看。', '只有去摘長豆，就在桃樹下。']\n",
            "['FIL AF-SA-what=PFV that.NOM rice OBL PI-see 2SG.GEN go LOC farm', 'NEG=IMPFV 1SG.NOM PI-see', 'PI-take PART XX 1SG.GEN this.OBL snap.bean below that.GEN XX peach']\n",
            "\n",
            "[\"su'elin kita u ma u ma-lingad-ay\", 'ira ku ccay a tamdaw ma-lingad tayni i lutuk', \"su'elin mi-u'uk sa ku 'ayam ma-nengneng nira hay uya tayal aku sa'an l<um>uwad=tu cingra a tayni i umah nira\"]\n",
            "['su’elin kita u==,\\\\ ma u ma-lingad-ay.\\\\', 'ira ku ccay a tamdaw.\\\\ ma-lingad tayni i lutuk.\\\\', 'su’elin mi-o’o==k,\\\\ sa ku ‘ayam.\\\\ ma-nengneng nira hay,\\\\ uya tayal aku sa’an,_ l<um>uwad=tu cingra a,\\\\ tayni i umah nira.\\\\']\n",
            "['We farmers are really hard-working.', 'There was a person who went to the mountain.', 'When chickens crowed, he thought \"ah, my work,\" and he got up and went to his farm.']\n",
            "['我們農夫阿，真的是(辛苦)。', '有一個人，去山上。', '當雞鳴的時候，他想起「啊，我的工作」，他就起身到他的田裡。']\n",
            "['really 1IPL.NOM CN FS CN AF-farm-AY', 'EXIST NOM one LNK person AF-farm AF.come LOC mountain', 'really AF-crow SA NOM chicken PF-see 3SG.GEN DM that work 1SG.GEN said.so <AF>get.up=PFV 3SG.NOM LNK AF.come LOC farm 3SG.GEN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check length of big list\n",
        "print(len(ntu_files_am))\n",
        "print(len(ntu_big_list))\n",
        "\n",
        "# Check to make sure all languages have the same number of lines\n",
        "for i in range(len(ntu_big_list)):\n",
        "  story = ntu_big_list[i]\n",
        "  if not (len(story[0]) == len(story[1]) == len(story[2]) == len(story[3]) == len(story[4])):\n",
        "    print(ntu_files_am[i])\n",
        "    print(len(story[0]))\n",
        "    print(len(story[1]))\n",
        "    print(len(story[2]))\n",
        "    print(len(story[3]))\n",
        "    print(len(story[4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9n7VI3ZS2jH",
        "outputId": "437f8c1c-5fa9-43fc-93ec-9fd4e72e0471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3UC2KffJaFT"
      },
      "outputs": [],
      "source": [
        "# Now our NTU data is ready for XML\n",
        "# Each source list contains multiple text lists\n",
        "# Each text list includes a list for each language\n",
        "# Each list for each language contains sentences\n",
        "\n",
        "# The format of ntu_big_list is as follows\n",
        "# ntu_big_list[story][language][sentence]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: XML\n"
      ],
      "metadata": {
        "id": "XnXf84jUu2JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import xml.dom.minidom"
      ],
      "metadata": {
        "id": "dnyJ_O2Ra6Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the code for putting both the Amis and English translations into XML\n",
        "# For each story, create a root and a TEXT subelement under root\n",
        "for i in range(len(ntu_big_list)):\n",
        "  root = ET.Element(\"root\")\n",
        "  text_element = ET.SubElement(root, \"TEXT\", {\"xml:lang\": \"amis1246\", \"source\": f\"ntu_{filenames[i]}\", \"audio\": f\"{filenames[i]}.mp3\"})\n",
        "\n",
        "  # For each sentence in the story, create the S element and its child elements\n",
        "  for j in range(len(ntu_big_list[i][0])):\n",
        "\n",
        "    # First get the FORM (Amis sentence in standard orthography)\n",
        "    s_element = ET.SubElement(text_element, \"S\", id=f\"S{j}\")\n",
        "    form_element = ET.SubElement(s_element, \"FORM\", kindOf=\"standard\")\n",
        "    std_sentence = ntu_big_list[i][0][j]\n",
        "    std_sentence = re.sub(r\"[-=<>]\", \"\", std_sentence)\n",
        "    form_element.text = std_sentence\n",
        "\n",
        "    # Now get the Amis in original orthography\n",
        "    form_element = ET.SubElement(s_element, \"FORM\", kindOf=\"original\")\n",
        "    form_element.text = ntu_big_list[i][1][j]\n",
        "\n",
        "    # Now get the English\n",
        "    transl_element = ET.SubElement(s_element, \"TRANSL\", {\"xml:lang\": \"stan1293\"})\n",
        "    transl_element.text = ntu_big_list[i][2][j]\n",
        "\n",
        "    # Now get the Chinese\n",
        "    transl_element = ET.SubElement(s_element, \"TRANSL\", {\"xml:lang\": \"mand1415\"})\n",
        "    transl_element.text = ntu_big_list[i][3][j]\n",
        "\n",
        "    # Now get the glossing\n",
        "    transl_element = ET.SubElement(s_element, \"TRANSL\", {\"xml:lang\": \"gloss\"})\n",
        "    ntu_big_list[i][4][j] = re.sub(\"\\s\\s+\", \" \", ntu_big_list[i][4][j])\n",
        "    transl_element.text = ntu_big_list[i][4][j]\n",
        "\n",
        "    # Now get the words (from the standard Amis sentences)\n",
        "    words = ntu_big_list[i][0][j].split()\n",
        "    glossing = ntu_big_list[i][4][j].split()\n",
        "\n",
        "    for k in range(len(words)):\n",
        "      w_element = ET.SubElement(s_element, \"W\", id=f\"S{j}W{k}\")\n",
        "      wform_element = ET.SubElement(w_element, \"FORM\")\n",
        "      std_word = words[k]\n",
        "      std_word = re.sub(r\"[-=<>]\", \"\", std_word)\n",
        "      std_word = re.sub(r\"[.!?]\", \"\", std_word)\n",
        "      wform_element.text = std_word\n",
        "\n",
        "      # Now get the morphemes for each word\n",
        "      # Usually separated by <>, =, -, or space\n",
        "      # Split on =, –, and space\n",
        "      morphs = re.split(r\"[=\\-\\s]\", words[k])\n",
        "      glosses = re.split(r\"[=\\-\\s]\", glossing[k])\n",
        "      # Deal with infixes\n",
        "      morph_match = re.search(r\"<.+>\", words[k])\n",
        "      gloss_match = re.search(r\"<.+>\", glossing[k])\n",
        "      if morph_match:\n",
        "        morph_infix = morph_match.group()\n",
        "        morph_infix_clean = re.sub(\"<\", \"\", morph_infix)\n",
        "        morph_infix_clean = re.sub(\">\", \"\", morph_infix_clean)\n",
        "        morphs.insert(0, morph_infix_clean)\n",
        "        for m in range(len(morphs)):\n",
        "          morphs[m] = re.sub(morph_infix, \"\", morphs[m])\n",
        "      if gloss_match:\n",
        "        gl_infix = gloss_match.group()\n",
        "        gl_infix_clean = re.sub(\"<\", \"\", gl_infix)\n",
        "        gl_infix_clean = re.sub(\">\", \"\", gl_infix_clean)\n",
        "        glosses.insert(0, gl_infix_clean)\n",
        "        for m in range(len(glosses)):\n",
        "          glosses[m] = re.sub(gl_infix, \"\", glosses[m])\n",
        "\n",
        "      for m in range(len(morphs)):\n",
        "        m_element = ET.SubElement(w_element, \"M\", id=f\"S{j}W{k}M{m}\")\n",
        "        # Create FORM within each morpheme\n",
        "        mform_element = ET.SubElement(m_element, \"FORM\")\n",
        "        mform_element.text = morphs[m]\n",
        "        # Create TRANS for gloss within morpheme\n",
        "        try:\n",
        "          glosstransl_element = ET.SubElement(m_element, \"TRANSL\")\n",
        "          glosstransl_element.text = glosses[m]\n",
        "        except:\n",
        "          print(filenames[i], \", Sentence:\", j, \", Word:\", k, \"Morpheme:\", m)\n",
        "\n",
        "  # Generate XML tree\n",
        "  tree = ET.ElementTree(root)\n",
        "\n",
        "  # This is what will make the XML look \"pretty\" (readable,\n",
        "  # with newlines, indenting) when you write it out to a file.\n",
        "  xml_str = xml.dom.minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"    \")\n",
        "\n",
        "  # This does the writing to the file.\n",
        "  with open(f\"/content/drive/MyDrive/Part1/NTUV4/ntu_{filenames[i]}.xml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(xml_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go6DpuA5soCY",
        "outputId": "617c1cc5-a03f-4665-b9aa-89828b9d9609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amis_Conv-talking_tamih_panay , Sentence: 19 , Word: 13 Morpheme: 1\n",
            "Amis_Conv-talking_tamih_panay , Sentence: 19 , Word: 13 Morpheme: 2\n",
            "Amis_Nr-frog_ofad , Sentence: 2 , Word: 2 Morpheme: 1\n",
            "Amis_Nr-intro_tamih , Sentence: 10 , Word: 2 Morpheme: 2\n"
          ]
        }
      ]
    }
  ]
}