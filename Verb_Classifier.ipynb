{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp9GMRwHKN3-"
      },
      "outputs": [],
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjxabUB29sh9"
      },
      "source": [
        "# STEP 1: Parse NTU Corpus XML and organize data\n",
        "In this section, we parse the NTU XML files taken from the NTUV4 folder of our Part1 folder, which we submitted for part 1 of this assignment. Using the glosses, we attempt to determine which words are verbs and we attempt to determine their voice. We create a list of all unique verbs, all words in each voice, and all verb stems that we have identified. We then write these lists to files in the Part2 folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTLubqxx4hSV"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import glob\n",
        "from operator import setitem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-si9OCQE4itf"
      },
      "outputs": [],
      "source": [
        "# Make a list of XML files for the NTU data\n",
        "xml_files = glob.glob(\"/content/drive/MyDrive/Part1/NTUV4/*\")\n",
        "print(xml_files[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4mXuGCf5xKz"
      },
      "outputs": [],
      "source": [
        "# Make list of glosses that indicate verbs\n",
        "# These glosses indicate the following:\n",
        "# agent focus, patient focus, location focus, instrument focus, perfective,\n",
        "# pi-, Consonant reduplication with /a/, ka-, sa-, causitive, a-\n",
        "verb_markers = [\"AF\", \"PF\", \"LF\", \"IF\", \"PFV\", \"PI\", \"Ca\", \"KA\", \"SA\", \"CAU\", \"A\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgEuqT8OPma4"
      },
      "outputs": [],
      "source": [
        "verb_list = []\n",
        "word_list = []\n",
        "gloss_list = []\n",
        "verb_stem_list = dict()\n",
        "\n",
        "av_list = []\n",
        "pv_list = []\n",
        "lv_list = []\n",
        "iv_list = []\n",
        "\n",
        "a_dict = dict()\n",
        "p_dict = dict()\n",
        "l_dict = dict()\n",
        "i_dict = dict()\n",
        "\n",
        "# For each XML file\n",
        "for xmlfile in xml_files:\n",
        "\n",
        "  # Read in and parse the XML file\n",
        "  tree = ET.parse(xmlfile)\n",
        "  root = tree.getroot()\n",
        "  file_words = []\n",
        "\n",
        "  for S in root.iter('S'):\n",
        "    sent_words = []\n",
        "\n",
        "    # For each word in the file\n",
        "    for W in S.iter('W'):\n",
        "\n",
        "      # Get the word\n",
        "      word = W.find('FORM').text\n",
        "\n",
        "      # Add word to list of words, except for XX\n",
        "      if word != \"XX\":\n",
        "        sent_words.append(word)\n",
        "\n",
        "      # Initialize flags for verb and voice\n",
        "      verb_check = 0\n",
        "      av_check = 0\n",
        "      pv_check = 0\n",
        "      lv_check = 0\n",
        "      iv_check = 0\n",
        "\n",
        "      sa = 0\n",
        "      an = 0\n",
        "      mi = 0\n",
        "      ma = 0\n",
        "      aw = 0\n",
        "      pi = 0\n",
        "      ka = 0\n",
        "\n",
        "      # Check for false start, lexical glosses, and the gloss \"Amis\"\n",
        "      fs = 0\n",
        "      lexical = 0\n",
        "      amis = 0\n",
        "\n",
        "      # For each morpheme in the word\n",
        "      for M in W.iter('M'):\n",
        "\n",
        "        # Get the gloss\n",
        "        gloss=M.find(\"TRANSL\").text\n",
        "        if gloss==None:\n",
        "          continue\n",
        "\n",
        "        # Add gloss to list of glosses\n",
        "        # We used this code to find out which glosses appear in the NTU Corpus\n",
        "        if gloss not in gloss_list:\n",
        "          gloss_list.append(gloss)\n",
        "\n",
        "        if gloss.islower():\n",
        "          stem=M.find(\"FORM\").text\n",
        "\n",
        "        # Check for certain markers that may indicate voice\n",
        "        # These morphemes appear in prefix-suffix sets in words\n",
        "        # So we need to check for combinations of them at the word level\n",
        "        # But we only have glosses by morpheme in the XML\n",
        "        # So we use these flags to track the markers on the word level\n",
        "        if re.search(r\"SA\", gloss):\n",
        "          sa = 1\n",
        "        if re.search(r\"AN\", gloss):\n",
        "          an = 1\n",
        "        if re.search(r\"MI\", gloss):\n",
        "          mi = 1\n",
        "        if re.search(r\"MA\", gloss):\n",
        "          ma = 1\n",
        "        if re.search(r\"AW\", gloss):\n",
        "          aw = 1\n",
        "        if re.search(r\"PI\", gloss):\n",
        "          pi = 1\n",
        "        if re.search(r\"KA\", gloss):\n",
        "          ka = 1\n",
        "        if re.search(r\"FS\", gloss):\n",
        "          fs = 1\n",
        "        if re.search(r\"[a-z]\", gloss):\n",
        "          lexical = 1\n",
        "        if gloss == \"Amis\":\n",
        "          amis = 1\n",
        "\n",
        "        # Check if the morpheme is a verb marker\n",
        "        for marker in verb_markers:\n",
        "          if re.search(marker,gloss):\n",
        "            verb_check=1\n",
        "\n",
        "        # Check if verb has SA and PI or SA and KA\n",
        "        # If it just has SA, it forms a superlative\n",
        "        # If it has SA with PI, KA, AN, or AW, it's a verb\n",
        "        if sa:\n",
        "          if pi or ka or an or aw:\n",
        "            verb_check=1\n",
        "          else:\n",
        "            verb_check=0\n",
        "\n",
        "        # Check if it's a nominalized verb (with suffix -ay)\n",
        "        # We don't want to count nominalized verbs as verbs\n",
        "        if re.search(r\"AY\",gloss):\n",
        "          verb_check=0\n",
        "\n",
        "        # Check which voice the word might fall into\n",
        "        # Our reasons for choosing these combinations of morphemes can be found in the notes sheet\n",
        "        # Check for AGENT VOICE\n",
        "        if re.search(r\"AF\", gloss) or (mi and not an):\n",
        "          av_check = 1\n",
        "\n",
        "        # Check for PATIENT VOICE\n",
        "        if re.search(r\"PF\", gloss) or (mi and an) or (sa and aw):\n",
        "          pv_check = 1\n",
        "\n",
        "        # Check for LOCATIVE VOICE\n",
        "        if re.search(r\"LF\", gloss) or (an and not mi):\n",
        "          lv_check = 1\n",
        "\n",
        "        # Check for INSTRUMENTAL VOICE\n",
        "        if re.search(r\"IF\", gloss) or (sa and pi) or (sa and ka):\n",
        "          iv_check = 1\n",
        "\n",
        "      # Add words to verb and voice lists\n",
        "\n",
        "      # Add to VERB list if\n",
        "      # it has verb markers, has a lexical gloss, and is not a false start\n",
        "      if verb_check == 1 and lexical and not fs and not amis:\n",
        "        verb_list.append(word)\n",
        "        # Add to stem list\n",
        "        if stem not in verb_stem_list:\n",
        "          verb_stem_list[stem]={word}\n",
        "        else:\n",
        "          verb_stem_list[stem].add(word)\n",
        "\n",
        "      # Add to AGENT VOICE list\n",
        "      if av_check == 1:\n",
        "        av_list.append(word)\n",
        "        if stem not in a_dict:\n",
        "          a_dict[stem]=1\n",
        "        else:\n",
        "          a_dict[stem]+=1\n",
        "\n",
        "      # Add to PATIENT VOICE list\n",
        "      if pv_check == 1:\n",
        "        pv_list.append(word)\n",
        "        if stem not in p_dict:\n",
        "          p_dict[stem]=1\n",
        "        else:\n",
        "          p_dict[stem]+=1\n",
        "\n",
        "      # Add to LOCATIVE VOICE list\n",
        "      if lv_check == 1:\n",
        "        lv_list.append(word)\n",
        "        if stem not in l_dict:\n",
        "          l_dict[stem]=1\n",
        "        else:\n",
        "          l_dict[stem]+=1\n",
        "\n",
        "      # Add to INSTRUMENTAL VOICE list\n",
        "      if iv_check == 1:\n",
        "        iv_list.append(word)\n",
        "        if stem not in i_dict:\n",
        "          i_dict[stem]=1\n",
        "        else:\n",
        "          i_dict[stem]+=1\n",
        "\n",
        "    # Add sentence of words to words in file\n",
        "    file_words.append(sent_words)\n",
        "\n",
        "  # Add word to word list\n",
        "  word_list.append(file_words)\n",
        "\n",
        "\n",
        "sorted_a_dict = dict(sorted(a_dict.items(), key=lambda x:x[1], reverse=True))\n",
        "sorted_p_dict = dict(sorted(p_dict.items(), key=lambda x:x[1], reverse=True))\n",
        "sorted_l_dict = dict(sorted(l_dict.items(), key=lambda x:x[1], reverse=True))\n",
        "sorted_i_dict = dict(sorted(i_dict.items(), key=lambda x:x[1], reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(verb_stem_list)\n",
        "print(len(verb_stem_list))\n",
        "\n",
        "print(sorted_a_dict)\n",
        "print(len(sorted_a_dict))\n",
        "print(sorted_p_dict)\n",
        "print(len(sorted_p_dict))\n",
        "print(sorted_l_dict)\n",
        "print(len(sorted_l_dict))\n",
        "print(sorted_i_dict)\n",
        "print(len(sorted_i_dict))"
      ],
      "metadata": {
        "id": "Cm9998lnA8ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_verb_stems = list(verb_stem_list.keys())\n",
        "print(just_verb_stems)"
      ],
      "metadata": {
        "id": "9LcszKVNRGnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN-ipqXSvCHN"
      },
      "outputs": [],
      "source": [
        "# We now have a nested list of the lists of all words in each of the sentences in each of the 18 NTU files\n",
        "# word_list = [[[all words in file 1 sentence 1], [words in file 1 sentence 2]], ...]\n",
        "\n",
        "# Print list of all words\n",
        "print(word_list)\n",
        "print(len(word_list))\n",
        "\n",
        "# And now we make a flattened list of all the words in all files\n",
        "word_list_joined = []\n",
        "for sublist in word_list:\n",
        "  for sub in sublist:\n",
        "    word_list_joined.extend(sub)\n",
        "print(len(word_list_joined))\n",
        "\n",
        "# Print list of all unique words\n",
        "unique_word = set(word_list_joined)\n",
        "print(unique_word)\n",
        "print(len(unique_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbLU5zxD6aQ5"
      },
      "outputs": [],
      "source": [
        "# Print list of words with verb markers\n",
        "print(verb_list)\n",
        "print(len(verb_list))\n",
        "\n",
        "# Print list of unique words with verb markers\n",
        "unique_verb = [element for element in set(verb_list)]\n",
        "print(unique_verb)\n",
        "print(len(unique_verb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwml96WfLjQK"
      },
      "outputs": [],
      "source": [
        "# Print list of words with agent voice\n",
        "print(set(av_list))\n",
        "print(len(set(av_list)))\n",
        "\n",
        "av_verbs=set(unique_verb).intersection(av_list)\n",
        "print(av_verbs)\n",
        "print(\"Number of AV verbs:\",len(av_verbs))\n",
        "# Print list of words with patient voice\n",
        "print(set(pv_list))\n",
        "print(len(set(pv_list)))\n",
        "\n",
        "pv_verbs=set(unique_verb).intersection(pv_list)\n",
        "print(pv_verbs)\n",
        "print(\"Number of PV verbs:\",len(pv_verbs))\n",
        "# Print list of words with locative voice\n",
        "print(set(lv_list))\n",
        "print(len(set(lv_list)))\n",
        "\n",
        "lv_verbs=set(unique_verb).intersection(lv_list)\n",
        "print(lv_verbs)\n",
        "print(\"Number of LV verbs:\",len(lv_verbs))\n",
        "# Print list of words with instrumental voice\n",
        "print(set(iv_list))\n",
        "print(len(set(iv_list)))\n",
        "\n",
        "iv_verbs=set(unique_verb).intersection(iv_list)\n",
        "print(iv_verbs)\n",
        "print(\"Number of IV verbs:\",len(iv_verbs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcGvbTEgKoeN"
      },
      "outputs": [],
      "source": [
        "# Print list of unique glosses\n",
        "print(gloss_list)\n",
        "print(len(gloss_list))\n",
        "\n",
        "new_gloss_list = []\n",
        "\n",
        "# Remove anything that is not an abbreviation (keep numbers)\n",
        "for gloss in gloss_list:\n",
        "  if \".\" in gloss:\n",
        "    split_gloss = gloss.split(\".\")\n",
        "    for g in split_gloss:\n",
        "      new_gloss_list.append(g)\n",
        "  else:\n",
        "    new_gloss_list.append(gloss)\n",
        "\n",
        "gloss_unique = sorted(set([g for g in new_gloss_list if g.isupper()]))\n",
        "print(gloss_unique)\n",
        "print(len(gloss_unique))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write all verbs from the NTU Corpus into a text file\n",
        "with open(\"/content/drive/MyDrive/Part2/Verbs/verbs_NTU.txt\", \"w\") as f:\n",
        "  f.write(\",\".join(verb_list))"
      ],
      "metadata": {
        "id": "6nz8ggVnECzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write each list of verbs by voice into a text file\n",
        "with open(\"/content/drive/MyDrive/Part2/Verbs/av_verbs_NTU.txt\", \"w\") as f:\n",
        "  f.write(\",\".join(av_verbs))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Part2/Verbs/pv_verbs_NTU.txt\", \"w\") as f:\n",
        "  f.write(\",\".join(pv_verbs))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Part2/Verbs/lv_verbs_NTU.txt\", \"w\") as f:\n",
        "  f.write(\",\".join(lv_verbs))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Part2/Verbs/iv_verbs_NTU.txt\", \"w\") as f:\n",
        "  f.write(\",\".join(iv_verbs))"
      ],
      "metadata": {
        "id": "5xhvl5YQFOGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write all verbs stems into a text file\n",
        "with open(\"/content/drive/MyDrive/Part2/Verbs/stems_NTU.txt\", \"w\") as f:\n",
        "  f.write(\",\".join(just_verb_stems))"
      ],
      "metadata": {
        "id": "Iy-JJJ0URZlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHuJOGeY9x78"
      },
      "source": [
        "# STEP 2: Create feature vectors\n",
        "\n",
        "In order to create a classifier for this data, we need to turn each word into a vector. We do this by assigning a unique index to each word in the NTU Corpus and creating a vector for each word that includes the following information: [ID of the word, ID of word before, ID of word two before, ID of word after, ID of word two after]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8INfr5h9XyU"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries that map each word to an ID, and vice versa\n",
        "# We can use this to go back and forth between IDs and characters\n",
        "word_to_idx = {word: idx+1 for idx, word in enumerate(unique_word)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puj5PzDc-xaF"
      },
      "outputs": [],
      "source": [
        "# Check to make sure it worked\n",
        "for i in range(1,6):\n",
        "  print(i, idx_to_word[i], word_to_idx[idx_to_word[i]])\n",
        "\n",
        "for i in range(1000,1006):\n",
        "  print(i, idx_to_word[i], word_to_idx[idx_to_word[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EPzs2vX_-5P"
      },
      "outputs": [],
      "source": [
        "# Now use the ditionaries we created to create feature vectors\n",
        "# We want the feature vectors to look like this:\n",
        "# [ID of word, ID of word before, ID of word two before, ID of word after, ID of word two after]\n",
        "\n",
        "# We want a list of lists of vectors with the following format:\n",
        "# all_vecs = [file_vecs for first file, file_vecs for second file, ...]\n",
        "# file_vecs = [sent_vecs for first sentence, sent_vecs for second file, ...]\n",
        "# sent_vecs = [vec for first word, vec for second word, ...]\n",
        "# vec = [ID of word, ID of word before, ID of word two before, ID of word after, ID of word two after]\n",
        "\n",
        "# We also want a list of whether or not each word is in our list of verbs (1 if verb, 0 if not)\n",
        "\n",
        "all_vecs = []\n",
        "all_is_verb = []\n",
        "\n",
        "# For each file in the corpus\n",
        "for word_sublist in word_list:\n",
        "  file_vecs = []\n",
        "  file_is_verb = []\n",
        "  # For each sentence in the file\n",
        "  for sentence in word_sublist:\n",
        "    sent_vecs = []\n",
        "    is_verb = []\n",
        "\n",
        "    # For each word in the sentence\n",
        "    for i in range(len(sentence)):\n",
        "      vec = []\n",
        "\n",
        "    # ID of word\n",
        "      vec.append(word_to_idx[sentence[i]])\n",
        "\n",
        "    # ID of word before\n",
        "      if i > 0:\n",
        "        vec.append(word_to_idx[sentence[i-1]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # ID of word two before\n",
        "      if i > 1:\n",
        "        vec.append(word_to_idx[sentence[i-2]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # ID of word after\n",
        "      if i < len(sentence) - 1:\n",
        "        vec.append(word_to_idx[sentence[i+1]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # ID of word two after\n",
        "      if i < len(sentence) - 2:\n",
        "        vec.append(word_to_idx[sentence[i+2]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # If word is a verb, 1, else 0\n",
        "      if sentence[i] in verb_list:\n",
        "        is_verb.append([1])\n",
        "      else:\n",
        "        is_verb.append([0])\n",
        "\n",
        "    # Append to list of vectors in sentence\n",
        "      sent_vecs.append(vec)\n",
        "\n",
        "    #Append\n",
        "    file_vecs.append(sent_vecs)\n",
        "    file_is_verb.append(is_verb)\n",
        "\n",
        "  # Append to big list of vectors\n",
        "  all_vecs.append(file_vecs)\n",
        "  # Append to big list of is_verb\n",
        "  all_is_verb.append(file_is_verb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzKVK4QLDYIv"
      },
      "outputs": [],
      "source": [
        "# Check the feature vectors and verb list\n",
        "\n",
        "# Print vectors for words in first file\n",
        "print(all_vecs[0])\n",
        "print(len(all_vecs))\n",
        "\n",
        "# Print verb/non-verb for words in first file\n",
        "print(all_is_verb[0])\n",
        "print(len(all_is_verb))\n",
        "\n",
        "# Print vectors for words in first sentence in first file\n",
        "print(all_vecs[0][0])\n",
        "print(len(all_vecs[0][0]))\n",
        "\n",
        "# Print verb/non-verb for words in first sentence in first file\n",
        "print(all_is_verb[0][0])\n",
        "print(len(all_is_verb[0][0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpsu8CQMjeqq"
      },
      "source": [
        "# STEP 3: Create a classifier for verb vs. non-verb\n",
        "Here we prepare the data to be used as train and test data. Then we experiment with four different types of classifiers: Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting. We made some adjustments to the verb list between attempts, so that accounts for the differences in accuracy between attempts. We get the best accuracy from the Gradient Boosting classifier, so that is the classifier that we will use to predict verbs in the Bible data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUb3b_NZjwEQ"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIYGqZ5SilbA"
      },
      "outputs": [],
      "source": [
        "# Prepare the data to use as train and test data\n",
        "\n",
        "# Initialize array\n",
        "np_vecs=numpy.array([[0,0,0,0,0]])\n",
        "\n",
        "# Flatten all_vecs list\n",
        "# For story in all_vecs\n",
        "for i in range(len(all_vecs)):\n",
        "  # For sentence in story\n",
        "  for j in range(len(all_vecs[i])):\n",
        "    # For word in sentence\n",
        "    for vec in all_vecs[i][j]:\n",
        "      np_vecs=numpy.append(np_vecs,[vec],axis=0)\n",
        "\n",
        "# Remove first element (all zeroes)\n",
        "np_vecs=np_vecs[1:]\n",
        "print(np_vecs)\n",
        "X=np_vecs\n",
        "\n",
        "np_verbs=numpy.array([0])\n",
        "\n",
        "# Flatten all_is_verb\n",
        "# For story in all_is_verb\n",
        "for i in range(len(all_is_verb)):\n",
        "  # For sentence in story\n",
        "  for j in range(len(all_is_verb[i])):\n",
        "    # For word in sentence\n",
        "    for is_verb in all_is_verb[i][j]:\n",
        "      np_verbs=numpy.append(np_verbs,[is_verb])\n",
        "print(np_verbs)\n",
        "# Remove first element (zero)\n",
        "y=np_verbs[1:]\n",
        "\n",
        "# Partition the data\n",
        "# We'll use 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixKunv6aQBn6"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression model\n",
        "logregmodel = LogisticRegression()\n",
        "\n",
        "# train model (i.e., fit X to y)\n",
        "logregmodel.fit(X_train,y_train)\n",
        "\n",
        "predictions = logregmodel.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKhzDmi-S9bY"
      },
      "source": [
        "\n",
        "First attempt:\n",
        "```\n",
        "# Accuracy: 0.7004\n",
        "```\n",
        "Second attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8287\n",
        "```\n",
        "Third attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8287\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoH7IgjKQrjP"
      },
      "outputs": [],
      "source": [
        "#Decision tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_decision = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_decision = accuracy_score(y_test, y_pred_decision)\n",
        "print(f\"Accuracy: {accuracy_decision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRu_fQuDTESB"
      },
      "source": [
        "\n",
        "First attempt:\n",
        "```\n",
        "# Accuracy: 0.8644179894179894\n",
        "```\n",
        "Second attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8458994708994709\n",
        "```\n",
        "Third attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8220899470899471\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU-YtrJYQ6rH"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "# Here we set a parameter for the number of trees (estimators).\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Accuracy: {accuracy_rf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjN_Gyfveuuh"
      },
      "outputs": [],
      "source": [
        "print(y_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pKz7JP5TI0N"
      },
      "source": [
        "First attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8498677248677249\n",
        "```\n",
        "Second attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8544973544973545\n",
        "```\n",
        "Third attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8571428571428571\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XwITuNiRARL"
      },
      "outputs": [],
      "source": [
        "# Gradient boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Initialize the DecisionTreeClassifier\n",
        "# Hint: try changing the parameters to see what happens!\n",
        "# We saw big improvements in class by changing the number of estimators.\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100,\n",
        "                                           learning_rate=0.1,\n",
        "                                           max_depth=3,\n",
        "                                           random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_gb = gb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Accuracy: {accuracy_gb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USIYrBalTQcT"
      },
      "source": [
        "First attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8333333333333334\n",
        "```\n",
        "\n",
        "Second attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8617724867724867\n",
        "```\n",
        "\n",
        "Third attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8703703703703703\n",
        "```\n",
        "Fourth attempt:\n",
        "\n",
        "```\n",
        "# Accuracy: 0.8716931216931217\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tESjreamtHY"
      },
      "source": [
        "# STEP 4: Running the classifier on the Bible data\n",
        "In this section, we prepare the Bible data in a similar way to how we prepared the NTU data before. We create a list of feature vectors and run the Gradient Boosting classifier on it. Then we write the list of verbs in the Bible data to a text file in the Part2 folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYAbZAFemwtt"
      },
      "outputs": [],
      "source": [
        "# Make a list of XML files\n",
        "bible_files = glob.glob(\"/content/drive/MyDrive/Part1/Bible/*\")\n",
        "print(bible_files[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzZRmff3mzey"
      },
      "outputs": [],
      "source": [
        "bible_word_list = []\n",
        "\n",
        "for biblefile in bible_files:\n",
        "\n",
        "  # Read in and parse the XML file\n",
        "  tree = ET.parse(biblefile)\n",
        "  root = tree.getroot()\n",
        "  bible_file_words = []\n",
        "\n",
        "  # For sentence in file\n",
        "  # The bible data is not split up by sentences, but by group of sentences\n",
        "  # So we need to split each sentence up\n",
        "  # And then split those sentences into words\n",
        "  for S in root.iter('S'):\n",
        "    bible_sent_words = []\n",
        "\n",
        "    # Get standardized set of sentences\n",
        "    sentences = S.find(\"FORM[@kindOf='standard']\").text\n",
        "    # Split on sentence-final punctuation to separate the sentences\n",
        "    sentences_list = re.split(r\"[.!?]\", sentences)\n",
        "\n",
        "    sents = []\n",
        "    # For each sentence in the list\n",
        "    for sent in sentences_list:\n",
        "      # Split into words\n",
        "      sents = sent.split()\n",
        "      # Append words to list\n",
        "      #sents.append(words)\n",
        "\n",
        "      # For each word in the file\n",
        "      for word in sents:\n",
        "\n",
        "        # Add word to list of words in sentence\n",
        "        bible_sent_words.append(word)\n",
        "\n",
        "      # Add sentence of words to words in file\n",
        "      bible_file_words.append(bible_sent_words)\n",
        "\n",
        "  # Add word to word list\n",
        "  bible_word_list.append(bible_file_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpC_Nv1Xm2uL"
      },
      "outputs": [],
      "source": [
        "# Print list of all words\n",
        "print(len(bible_word_list))\n",
        "# Print first sentence of first file\n",
        "print(bible_word_list[0][0])\n",
        "# Print first word of first file\n",
        "print(bible_word_list[0][0][0])\n",
        "\n",
        "# And now we make a flattened list of all the words in all files\n",
        "bible_word_list_joined = []\n",
        "for sublist in bible_word_list:\n",
        "  for sub in sublist:\n",
        "    bible_word_list_joined.extend(sub)\n",
        "print(len(bible_word_list_joined))\n",
        "\n",
        "# Print list of all unique words\n",
        "bible_unique_word = set(bible_word_list_joined)\n",
        "print(bible_unique_word)\n",
        "print(len(bible_unique_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwsRELNum6ZG"
      },
      "outputs": [],
      "source": [
        "# Extend the dictionaries that map each word to an ID, and vice versa\n",
        "# These should include the words in the original dictionaries (from NTU) plus the words in the bible\n",
        "# We can use this to go back and forth between IDs and characters\n",
        "all_word_to_idx = word_to_idx\n",
        "all_idx_to_word = idx_to_word\n",
        "offset = len(unique_word)\n",
        "idx = 0\n",
        "\n",
        "for word in bible_unique_word:\n",
        "  if word not in unique_word:\n",
        "    idx += 1\n",
        "    all_word_to_idx[word] = int(idx) + offset\n",
        "    all_idx_to_word[int(idx) + offset] = word\n",
        "\n",
        "\n",
        "# Check to make sure it worked\n",
        "for i in range(1,6):\n",
        "  print(i, idx_to_word[i], word_to_idx[idx_to_word[i]])\n",
        "  print(i, all_idx_to_word[i], all_word_to_idx[all_idx_to_word[i]])\n",
        "\n",
        "\n",
        "for i in range(offset-6,offset):\n",
        "  print(i, idx_to_word[i], word_to_idx[idx_to_word[i]])\n",
        "  print(i, all_idx_to_word[i], all_word_to_idx[idx_to_word[i]])\n",
        "\n",
        "\n",
        "for i in range(5000,5006):\n",
        "  print(i, all_idx_to_word[i], all_word_to_idx[all_idx_to_word[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTqGgYDQm8c3"
      },
      "outputs": [],
      "source": [
        "# Now use the ditionaries we created to create feature vectors\n",
        "# We want the feature vectors to look like this:\n",
        "# [ID of word, ID of word before, ID of word two before, ID of word after, ID of word two after]\n",
        "\n",
        "# We want a list of lists of vectors with the following format:\n",
        "# all_vecs = [file_vecs for first file, file_vecs for second file, ...]\n",
        "# file_vecs = [sent_vecs for first sentence, sent_vecs for second file, ...]\n",
        "# sent_vecs = [vec for first word, vec for second word, ...]\n",
        "# vec = [ID of word, ID of word before, ID of word two before, ID of word after, ID of word two after]\n",
        "\n",
        "# We also want a list of whether or not each word is in our list of verbs (1 if verb, 0 if not)\n",
        "\n",
        "bible_all_vecs = []\n",
        "\n",
        "# For each file in the corpus\n",
        "for word_sublist in bible_word_list:\n",
        "  bible_file_vecs = []\n",
        "  bible_file_is_verb = []\n",
        "  # For each sentence in the file\n",
        "  for sentence in word_sublist:\n",
        "    bible_sent_vecs = []\n",
        "\n",
        "    # For each word in the sentence\n",
        "    for i in range(len(sentence)):\n",
        "      vec = []\n",
        "\n",
        "    # ID of word\n",
        "      vec.append(all_word_to_idx[sentence[i]])\n",
        "\n",
        "    # ID of word before\n",
        "      if i > 0:\n",
        "        vec.append(all_word_to_idx[sentence[i-1]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # ID of word two before\n",
        "      if i > 1:\n",
        "        vec.append(all_word_to_idx[sentence[i-2]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # ID of word after\n",
        "      if i < len(sentence) - 1:\n",
        "        vec.append(all_word_to_idx[sentence[i+1]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # ID of word two after\n",
        "      if i < len(sentence) - 2:\n",
        "        vec.append(all_word_to_idx[sentence[i+2]])\n",
        "      else:\n",
        "        vec.append(0)\n",
        "\n",
        "    # Append to list of vectors in sentence\n",
        "      bible_sent_vecs.append(vec)\n",
        "\n",
        "    # Append to list of sentences in file\n",
        "    bible_file_vecs.append(bible_sent_vecs)\n",
        "\n",
        "  # Append to big list of vectors\n",
        "  bible_all_vecs.append(bible_file_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjgucymvm_yZ"
      },
      "outputs": [],
      "source": [
        "# Check the feature vectors and verb list\n",
        "# Print vectors for words in first file\n",
        "print(bible_all_vecs[0])\n",
        "print(len(bible_all_vecs))\n",
        "\n",
        "# Print vectors for words in first sentence in first file\n",
        "print(bible_all_vecs[0][0])\n",
        "print(len(bible_all_vecs[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMKy8QHMyT_9"
      },
      "outputs": [],
      "source": [
        "# Initialize array\n",
        "bible_np_vecs=numpy.array([[0,0,0,0,0]])\n",
        "\n",
        "# Flatten all_vecs list\n",
        "# For file in all_vecs\n",
        "for i in range(len(bible_all_vecs)):\n",
        "  # For sentence in file\n",
        "  for j in range(len(bible_all_vecs[i])):\n",
        "    # For word in sentence\n",
        "    for vec in bible_all_vecs[i][j]:\n",
        "      bible_np_vecs=numpy.append(bible_np_vecs,[vec],axis=0)\n",
        "\n",
        "# Remove first element (all zeroes)\n",
        "bible_np_vecs=bible_np_vecs[1:]\n",
        "print(bible_np_vecs)\n",
        "X=bible_np_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o_vdUXDxVD1"
      },
      "outputs": [],
      "source": [
        "# The gradient boosting model had the highest accuracy on the NTU test data\n",
        "# We will use this model to predict verbs in the Bible data\n",
        "gb_bible_pred = gb_classifier.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to make sure it's not all zeros\n",
        "print(gb_bible_pred[:1000])"
      ],
      "metadata": {
        "id": "-FTFucf4Iftg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of potential verbs in the Bible from the classifier's output\n",
        "bible_verbs = []\n",
        "\n",
        "for i in range(len(gb_bible_pred)):\n",
        "  if gb_bible_pred[i]:\n",
        "    verb = all_idx_to_word[X[i][0]]\n",
        "    bible_verbs.append(verb)"
      ],
      "metadata": {
        "id": "XiHN6No1F0Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(bible_verbs))"
      ],
      "metadata": {
        "id": "A2vEufJYHHQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the list of potential verbs in the Bible to a text file\n",
        "with open(\"/content/drive/MyDrive/Part2/Verbs/verbs_Bible.txt\", \"w\") as f:\n",
        "  f.write(\",\".join(bible_verbs))"
      ],
      "metadata": {
        "id": "F94cp9nFHEaI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}